import json

notebook_path = r"c:\Users\Junior.NOUMONVI\Documents\conversation\conversation_ia_qwen.ipynb"

# 1. DEFINITION DU CODE CORRECT

# chat_service.py (AVEC METRIQUES)
chat_service_code = [
    "%%writefile chat_service.py\n",
    "from typing import List, Tuple, Optional\n",
    "from models import Message\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "# CONFIGURATION CACHE\n",
    "CACHE_DIR = \"/content/drive/MyDrive/huggingface_cache\"\n",
    "os.environ['HF_HOME'] = CACHE_DIR\n",
    "os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR\n",
    "\n",
    "class ChatService:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.embedding_model = None\n",
    "\n",
    "    def load_models(self):\n",
    "        if not os.path.exists(CACHE_DIR):\n",
    "            os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "            print(f\"üìÅ Dossier de cache cr√©√© : {CACHE_DIR}\")\n",
    "\n",
    "        if self.model is None:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "            print(f\"üîÑ V√©rification du mod√®le Qwen dans {CACHE_DIR}...\")\n",
    "\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                cache_dir=CACHE_DIR\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                cache_dir=CACHE_DIR\n",
    "            )\n",
    "            print(\"‚úÖ Qwen 7B charg√©\")\n",
    "\n",
    "        if self.embedding_model is None:\n",
    "            print(\"üîÑ Chargement mod√®le embeddings...\")\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=CACHE_DIR)\n",
    "            print(\"‚úÖ Mod√®le embeddings charg√©\")\n",
    "\n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        if self.embedding_model is None:\n",
    "            self.load_models()\n",
    "        return self.embedding_model.encode(text).tolist()\n",
    "\n",
    "    async def generate_response_stream(self, history: List[Message]):\n",
    "        from transformers import TextIteratorStreamer\n",
    "        from threading import Thread\n",
    "\n",
    "        if self.model is None:\n",
    "            self.load_models()\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant IA intelligent et amical. R√©ponds de mani√®re naturelle et concise en fran√ßais.\"}\n",
    "        ]\n",
    "        for msg in history:\n",
    "            role = \"user\" if msg.sender == \"user\" else \"assistant\"\n",
    "            messages.append({\"role\": role, \"content\": msg.content})\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=1024).to(self.model.device)\n",
    "\n",
    "        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        generation_kwargs = dict(\n",
    "            inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        token_count = 0\n",
    "        first_token_received = False\n",
    "\n",
    "        for new_text in streamer:\n",
    "            if not first_token_received:\n",
    "                latency = time.time() - start_time\n",
    "                print(f\"\\n‚è±Ô∏è LATENCE (Temps avant 1er mot) : {latency:.4f} secondes\")\n",
    "                first_token_received = True\n",
    "\n",
    "            token_count += 1\n",
    "            yield new_text\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        tokens_per_sec = token_count / total_time if total_time > 0 else 0\n",
    "\n",
    "        print(f\"‚ö° VITESSE DE GENERATION : {tokens_per_sec:.2f} tokens/seconde\")\n",
    "        print(f\"üìù Total tokens (approx stream) : {token_count}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    async def generate_response(self, history: List[Message]) -> Tuple[str, Optional[List[str]]]:\n",
    "        full_response = \"\"\n",
    "        async for token in self.generate_response_stream(history):\n",
    "            full_response += token\n",
    "        return full_response, None\n",
    "\n",
    "chat_service = ChatService()"
]

# chat_router.py (AVEC TOUTES LES ROUTES)
chat_router_code = [
    "%%writefile chat_router.py\n",
    "from fastapi import APIRouter, HTTPException, UploadFile, File\n",
    "from fastapi.responses import StreamingResponse\n",
    "from typing import List\n",
    "from schemas import ConversationCreate, ConversationResponse, MessageCreate, MessageResponse\n",
    "from models import Message\n",
    "from history_service import HistoryService\n",
    "from chat_service import chat_service\n",
    "import pypdf\n",
    "import io\n",
    "import time\n",
    "\n",
    "router = APIRouter()\n",
    "history_service = HistoryService()\n",
    "\n",
    "@router.post(\"/conversations\", response_model=ConversationResponse)\n",
    "async def create_conversation(conversation: ConversationCreate):\n",
    "    conv = await history_service.create_conversation(conversation)\n",
    "\n",
    "    if conversation.mode == \"ai_initiated\":\n",
    "        greeting = \"Bonjour ! Je suis votre assistant IA. Comment puis-je vous aider aujourd'hui ?\"\n",
    "        greeting_embedding = chat_service.generate_embedding(greeting)\n",
    "        await history_service.add_message(conv.id, \"ai\", greeting, greeting_embedding, None)\n",
    "        conv = await history_service.get_conversation(conv.id)\n",
    "\n",
    "    return ConversationResponse(\n",
    "        id=conv.id,\n",
    "        title=conv.title,\n",
    "        mode=conv.mode,\n",
    "        created_at=conv.created_at,\n",
    "        messages=[\n",
    "            MessageResponse(\n",
    "                id=m.id,\n",
    "                sender=m.sender,\n",
    "                content=m.content,\n",
    "                timestamp=m.timestamp,\n",
    "                suggestions=m.suggestions\n",
    "            ) for m in conv.messages\n",
    "        ]\n",
    "    )\n",
    "\n",
    "@router.get(\"/conversations\", response_model=List[ConversationResponse])\n",
    "async def list_conversations():\n",
    "    convs = await history_service.list_conversations()\n",
    "    return [\n",
    "        ConversationResponse(\n",
    "            id=c.id,\n",
    "            title=c.title,\n",
    "            mode=c.mode,\n",
    "            created_at=c.created_at,\n",
    "            messages=[]\n",
    "        ) for c in convs\n",
    "    ]\n",
    "\n",
    "@router.get(\"/conversations/{conversation_id}\", response_model=ConversationResponse)\n",
    "async def get_conversation(conversation_id: str):\n",
    "    conv = await history_service.get_conversation(conversation_id)\n",
    "    if not conv:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "\n",
    "    return ConversationResponse(\n",
    "        id=conv.id,\n",
    "        title=conv.title,\n",
    "        mode=conv.mode,\n",
    "        created_at=conv.created_at,\n",
    "        messages=[\n",
    "            MessageResponse(\n",
    "                id=m.id,\n",
    "                sender=m.sender,\n",
    "                content=m.content,\n",
    "                timestamp=m.timestamp,\n",
    "    return MessageResponse(\n",
    "        id=ai_msg.id,\n",
    "        sender=ai_msg.sender,\n",
    "        content=ai_msg.content,\n",
    "        timestamp=ai_msg.timestamp,\n",
    "        suggestions=ai_msg.suggestions\n",
    "    )\n",
    "\n",
    "@router.delete(\"/conversations/{conversation_id}\")\n",
    "async def delete_conversation(conversation_id: str):\n",
    "    conv = await history_service.get_conversation(conversation_id)\n",
    "    if not conv:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "\n",
    "    await history_service.delete_conversation(conversation_id)\n",
    "    return {\"message\": \"Conversation supprim√©e\"}\n",
    "\n",
    "@router.patch(\"/conversations/{conversation_id}\")\n",
    "async def rename_conversation(conversation_id: str, title: str):\n",
    "    conv = await history_service.get_conversation(conversation_id)\n",
    "    if not conv:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "\n",
    "    updated = await history_service.rename_conversation(conversation_id, title)\n",
    "    return ConversationResponse(\n",
    "        id=updated.id,\n",
    "        title=updated.title,\n",
    "        mode=updated.mode,\n",
    "        created_at=updated.created_at,\n",
    "        messages=[\n",
    "            MessageResponse(\n",
    "                id=m.id,\n",
    "                sender=m.sender,\n",
    "                content=m.content,\n",
    "                timestamp=m.timestamp,\n",

# Ins√©rer les nouvelles cellules avant la cellule de d√©marrage (main.py ou uvicorn)
# On cherche l'index de main.py
insert_idx = len(new_cells) - 1
for i, cell in enumerate(new_cells):
    if cell['cell_type'] == 'code' and cell['source'] and '%%writefile main.py' in cell['source'][0]:
        insert_idx = i
        break
    
# Injecter l'installation de pypdf dans la premi√®re cellule
if nb['cells'][0]['cell_type'] == 'code':
    source = nb['cells'][0]['source']
    if "!pip install -q pypdf" not in "".join(source):
        nb['cells'][0]['source'].insert(0, "!pip install -q pypdf\n")

# Cr√©er les cellules
cell_service = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": chat_service_code
}

cell_router = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": chat_router_code
}

# Ins√©rer
new_cells.insert(insert_idx, cell_router)
new_cells.insert(insert_idx, cell_service)

nb['cells'] = new_cells

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2)

print("‚úÖ Notebook patched: Duplicates removed, correct code injected.")
