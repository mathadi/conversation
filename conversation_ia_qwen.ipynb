{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZKR4kfmkv5E",
        "outputId": "95c4725c-acdc-4eb9-d553-79590efdaf63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u2705 D\u00e9pendances install\u00e9es\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fastapi uvicorn pyngrok chromadb sentence-transformers\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install -q pydantic-settings python-dotenv httpx\n",
        "print('\u2705 D\u00e9pendances install\u00e9es')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8x2l12Pk4gZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e079f2-1077-4435-c32b-d9017e67e2f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u2705 Google Drive mont\u00e9\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('\u2705 Google Drive mont\u00e9')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOFO8cGpk9Ed",
        "outputId": "421d4504-8c6f-48a8-cd6c-6f8178081929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "ngrok is already the newest version (3.33.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "\u2705 ngrok install\u00e9 et configur\u00e9\n"
          ]
        }
      ],
      "source": [
        "# Installer ngrok via snap (plus fiable sur Colab)\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update -qq\n",
        "!sudo apt install ngrok -qq\n",
        "\n",
        "# Configurer le token\n",
        "!ngrok authtoken 361IzWAOvuvUMfJjCWN0hOTPALb_7SPYyHQrzZkdHiXtNrnME\n",
        "\n",
        "# Tuer les processus ngrok existants\n",
        "!pkill -f ngrok\n",
        "\n",
        "print('\u2705 ngrok install\u00e9 et configur\u00e9')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HALHW6M8lDL7",
        "outputId": "8b789c7a-e4c5-44e0-d9e2-277fa5c718c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting database.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile database.py\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "DB_PATH = \"/content/drive/MyDrive/conversation_db\"\n",
        "\n",
        "client = chromadb.PersistentClient(\n",
        "    path=DB_PATH,\n",
        "    settings=Settings(anonymized_telemetry=False)\n",
        ")\n",
        "\n",
        "conversations_collection = client.get_or_create_collection(\n",
        "    name=\"conversations\",\n",
        "    metadata={\"description\": \"Conversations metadata\"}\n",
        ")\n",
        "\n",
        "messages_collection = client.get_or_create_collection(\n",
        "    name=\"messages\",\n",
        "    metadata={\"description\": \"Messages with embeddings\"}\n",
        ")\n",
        "\n",
        "print(f\"\u2705 ChromaDB initialis\u00e9: {DB_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ7qSs6NlGwy",
        "outputId": "cea3c01f-dc94-458c-c617-bdbed2de83fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class Conversation:\n",
        "    id: str\n",
        "    title: str\n",
        "    mode: str\n",
        "    created_at: str\n",
        "    messages: List['Message'] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    id: str\n",
        "    conversation_id: str\n",
        "    sender: str\n",
        "    content: str\n",
        "    timestamp: str\n",
        "    suggestions: Optional[List[str]] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfRsXOs9lc3d",
        "outputId": "da34a86f-d6f0-4022-a6b5-efbe2f31fee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting schemas.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile schemas.py\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "\n",
        "class MessageCreate(BaseModel):\n",
        "    content: str\n",
        "    stream: bool = False\n",
        "\n",
        "class MessageResponse(BaseModel):\n",
        "    id: str\n",
        "    sender: str\n",
        "    content: str\n",
        "    timestamp: str\n",
        "    suggestions: Optional[List[str]] = None\n",
        "\n",
        "class ConversationCreate(BaseModel):\n",
        "    mode: str = \"user_initiated\"\n",
        "    title: Optional[str] = \"New Conversation\"\n",
        "\n",
        "class ConversationResponse(BaseModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    mode: str\n",
        "    created_at: str\n",
        "    messages: List[MessageResponse] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfmiWeoSlhFE",
        "outputId": "33553c74-692a-4b7a-bd53-b561bcc8cf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting history_service.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile history_service.py\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "from models import Conversation, Message\n",
        "from schemas import ConversationCreate\n",
        "from database import conversations_collection, messages_collection\n",
        "\n",
        "class HistoryService:\n",
        "    async def create_conversation(self, conversation_data: ConversationCreate) -> Conversation:\n",
        "        conv_id = str(uuid.uuid4())\n",
        "        created_at = datetime.utcnow().isoformat()\n",
        "\n",
        "        conversations_collection.add(\n",
        "            ids=[conv_id],\n",
        "            metadatas=[{\n",
        "                \"title\": conversation_data.title,\n",
        "                \"mode\": conversation_data.mode,\n",
        "                \"created_at\": created_at\n",
        "            }],\n",
        "            documents=[f\"{conversation_data.title}\"]\n",
        "        )\n",
        "\n",
        "        return Conversation(\n",
        "            id=conv_id,\n",
        "            title=conversation_data.title,\n",
        "            mode=conversation_data.mode,\n",
        "            created_at=created_at,\n",
        "            messages=[]\n",
        "        )\n",
        "\n",
        "    async def get_conversation(self, conversation_id: str) -> Conversation:\n",
        "        conv_result = conversations_collection.get(ids=[conversation_id])\n",
        "        if not conv_result['ids']:\n",
        "            return None\n",
        "\n",
        "        metadata = conv_result['metadatas'][0]\n",
        "        messages = await self.get_messages(conversation_id)\n",
        "\n",
        "        return Conversation(\n",
        "            id=conversation_id,\n",
        "            title=metadata['title'],\n",
        "            mode=metadata['mode'],\n",
        "            created_at=metadata['created_at'],\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "    async def add_message(self, conversation_id: str, sender: str, content: str,\n",
        "                         embedding: List[float], suggestions=None) -> Message:\n",
        "        msg_id = str(uuid.uuid4())\n",
        "        timestamp = datetime.utcnow().isoformat()\n",
        "\n",
        "        metadata = {\n",
        "            \"conversation_id\": conversation_id,\n",
        "            \"sender\": sender,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "\n",
        "        if suggestions and len(suggestions) > 0:\n",
        "            metadata[\"suggestions\"] = \",\".join(suggestions)\n",
        "\n",
        "        messages_collection.add(\n",
        "            ids=[msg_id],\n",
        "            embeddings=[embedding],\n",
        "            metadatas=[metadata],\n",
        "            documents=[content]\n",
        "        )\n",
        "\n",
        "        return Message(\n",
        "            id=msg_id,\n",
        "            conversation_id=conversation_id,\n",
        "            sender=sender,\n",
        "            content=content,\n",
        "            timestamp=timestamp,\n",
        "            suggestions=suggestions\n",
        "        )\n",
        "\n",
        "    async def get_messages(self, conversation_id: str) -> List[Message]:\n",
        "        results = messages_collection.get(\n",
        "            where={\"conversation_id\": conversation_id}\n",
        "        )\n",
        "\n",
        "        messages = []\n",
        "        for i, msg_id in enumerate(results['ids']):\n",
        "            metadata = results['metadatas'][i]\n",
        "            sugg_str = metadata.get('suggestions')\n",
        "            suggestions = sugg_str.split(',') if sugg_str else None\n",
        "\n",
        "            messages.append(Message(\n",
        "                id=msg_id,\n",
        "                conversation_id=conversation_id,\n",
        "                sender=metadata['sender'],\n",
        "                content=results['documents'][i],\n",
        "                timestamp=metadata['timestamp'],\n",
        "                suggestions=suggestions\n",
        "            ))\n",
        "\n",
        "        messages.sort(key=lambda x: x.timestamp)\n",
        "        return messages\n",
        "\n",
        "    async def list_conversations(self, skip: int = 0, limit: int = 100) -> List[Conversation]:\n",
        "        results = conversations_collection.get()\n",
        "\n",
        "        conversations = []\n",
        "        for i, conv_id in enumerate(results['ids']):\n",
        "            metadata = results['metadatas'][i]\n",
        "            conversations.append(Conversation(\n",
        "                id=conv_id,\n",
        "                title=metadata['title'],\n",
        "                mode=metadata['mode'],\n",
        "                created_at=metadata['created_at'],\n",
        "                messages=[]\n",
        "            ))\n",
        "\n",
        "        conversations.sort(key=lambda x: x.created_at, reverse=True)\n",
        "        return conversations[skip:skip+limit]\n",
        "\n",
        "    async def delete_conversation(self, conversation_id: str):\n",
        "        # Supprimer les messages\n",
        "        messages_collection.delete(where={\"conversation_id\": conversation_id})\n",
        "        # Supprimer la conversation\n",
        "        conversations_collection.delete(ids=[conversation_id])\n",
        "\n",
        "    async def rename_conversation(self, conversation_id: str, title: str) -> Conversation:\n",
        "        conv = await self.get_conversation(conversation_id)\n",
        "        if conv:\n",
        "            conversations_collection.update(\n",
        "                ids=[conversation_id],\n",
        "                metadatas=[{\n",
        "                    \"title\": title,\n",
        "                    \"mode\": conv.mode,\n",
        "                    \"created_at\": conv.created_at\n",
        "                }],\n",
        "                documents=[title]\n",
        "            )\n",
        "            conv.title = title\n",
        "        return conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile chat_service.py\n",
        "from typing import List, Tuple, Optional\n",
        "from models import Message\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# CONFIGURATION CACHE\n",
        "CACHE_DIR = \"/content/drive/MyDrive/huggingface_cache\"\n",
        "os.environ['HF_HOME'] = CACHE_DIR\n",
        "os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR\n",
        "\n",
        "class ChatService:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.embedding_model = None\n",
        "\n",
        "    def load_models(self):\n",
        "        if not os.path.exists(CACHE_DIR):\n",
        "            os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "            print(f\"\ud83d\udcc1 Dossier de cache cr\u00e9\u00e9 : {CACHE_DIR}\")\n",
        "\n",
        "        if self.model is None:\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "\n",
        "            print(f\"\ud83d\udd04 V\u00e9rification du mod\u00e8le Qwen dans {CACHE_DIR}...\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                cache_dir=CACHE_DIR\n",
        "            )\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                cache_dir=CACHE_DIR\n",
        "            )\n",
        "            print(\"\u2705 Qwen 7B charg\u00e9\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            print(\"\ud83d\udd04 Chargement mod\u00e8le embeddings...\")\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=CACHE_DIR)\n",
        "            print(\"\u2705 Mod\u00e8le embeddings charg\u00e9\")\n",
        "\n",
        "    def generate_embedding(self, text: str) -> List[float]:\n",
        "        if self.embedding_model is None:\n",
        "            self.load_models()\n",
        "        return self.embedding_model.encode(text).tolist()\n",
        "\n",
        "    async def generate_response_stream(self, history: List[Message]):\n",
        "        from transformers import TextIteratorStreamer\n",
        "        from threading import Thread\n",
        "\n",
        "        if self.model is None:\n",
        "            self.load_models()\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Tu es un assistant IA intelligent et amical. R\u00e9ponds de mani\u00e8re naturelle et concise en fran\u00e7ais.\"}\n",
        "        ]\n",
        "        for msg in history:\n",
        "            role = \"user\" if msg.sender == \"user\" else \"assistant\"\n",
        "            messages.append({\"role\": role, \"content\": msg.content})\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=1024).to(self.model.device)\n",
        "\n",
        "        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "        generation_kwargs = dict(\n",
        "            inputs,\n",
        "            streamer=streamer,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        token_count = 0\n",
        "        first_token_received = False\n",
        "\n",
        "        for new_text in streamer:\n",
        "            if not first_token_received:\n",
        "                latency = time.time() - start_time\n",
        "                print(f\"\\n\u23f1\ufe0f LATENCE (Temps avant 1er mot) : {latency:.4f} secondes\")\n",
        "                first_token_received = True\n",
        "\n",
        "            token_count += 1\n",
        "            yield new_text\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time = end_time - start_time\n",
        "        tokens_per_sec = token_count / total_time if total_time > 0 else 0\n",
        "\n",
        "        print(f\"\u26a1 VITESSE DE GENERATION : {tokens_per_sec:.2f} tokens/seconde\")\n",
        "        print(f\"\ud83d\udcdd Total tokens (approx stream) : {token_count}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    async def generate_response(self, history: List[Message]) -> Tuple[str, Optional[List[str]]]:\n",
        "        full_response = \"\"\n",
        "        async for token in self.generate_response_stream(history):\n",
        "            full_response += token\n",
        "        return full_response, None\n",
        "\n",
        "chat_service = ChatService()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile chat_router.py\n",
        "from fastapi import APIRouter, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from typing import List\n",
        "from schemas import ConversationCreate, ConversationResponse, MessageCreate, MessageResponse\n",
        "from history_service import HistoryService\n",
        "from chat_service import chat_service\n",
        "\n",
        "router = APIRouter()\n",
        "history_service = HistoryService()\n",
        "\n",
        "@router.post(\"/conversations\", response_model=ConversationResponse)\n",
        "async def create_conversation(conversation: ConversationCreate):\n",
        "    conv = await history_service.create_conversation(conversation)\n",
        "\n",
        "    if conversation.mode == \"ai_initiated\":\n",
        "        greeting = \"Bonjour ! Je suis votre assistant IA. Comment puis-je vous aider aujourd'hui ?\"\n",
        "        greeting_embedding = chat_service.generate_embedding(greeting)\n",
        "        await history_service.add_message(conv.id, \"ai\", greeting, greeting_embedding, None)\n",
        "        conv = await history_service.get_conversation(conv.id)\n",
        "\n",
        "    return ConversationResponse(\n",
        "        id=conv.id,\n",
        "        title=conv.title,\n",
        "        mode=conv.mode,\n",
        "        created_at=conv.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in conv.messages\n",
        "        ]\n",
        "    )\n",
        "\n",
        "@router.get(\"/conversations\", response_model=List[ConversationResponse])\n",
        "async def list_conversations():\n",
        "    convs = await history_service.list_conversations()\n",
        "    return [\n",
        "        ConversationResponse(\n",
        "            id=c.id,\n",
        "            title=c.title,\n",
        "            mode=c.mode,\n",
        "            created_at=c.created_at,\n",
        "            messages=[]\n",
        "        ) for c in convs\n",
        "    ]\n",
        "\n",
        "@router.get(\"/conversations/{conversation_id}\", response_model=ConversationResponse)\n",
        "async def get_conversation(conversation_id: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    return ConversationResponse(\n",
        "        id=conv.id,\n",
        "        title=conv.title,\n",
        "        mode=conv.mode,\n",
        "        created_at=conv.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in conv.messages\n",
        "        ]\n",
        "    )\n",
        "\n",
        "@router.post(\"/conversations/{conversation_id}/messages\", response_model=MessageResponse)\n",
        "async def send_message(conversation_id: str, message: MessageCreate):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    user_embedding = chat_service.generate_embedding(message.content)\n",
        "    user_msg = await history_service.add_message(conversation_id, \"user\", message.content, user_embedding, None)\n",
        "\n",
        "    history = await history_service.get_messages(conversation_id)\n",
        "\n",
        "    if message.stream:\n",
        "        async def event_generator():\n",
        "            full_response = \"\"\n",
        "            async for token in chat_service.generate_response_stream(history):\n",
        "                full_response += token\n",
        "                yield token\n",
        "            \n",
        "            ai_embedding = chat_service.generate_embedding(full_response)\n",
        "            await history_service.add_message(conversation_id, \"ai\", full_response, ai_embedding, None)\n",
        "\n",
        "        return StreamingResponse(event_generator(), media_type=\"text/plain\")\n",
        "\n",
        "    ai_response, suggestions = await chat_service.generate_response(history)\n",
        "\n",
        "    ai_embedding = chat_service.generate_embedding(ai_response)\n",
        "    ai_msg = await history_service.add_message(conversation_id, \"ai\", ai_response, ai_embedding, None)\n",
        "\n",
        "    return MessageResponse(\n",
        "        id=ai_msg.id,\n",
        "        sender=ai_msg.sender,\n",
        "        content=ai_msg.content,\n",
        "        timestamp=ai_msg.timestamp,\n",
        "        suggestions=ai_msg.suggestions\n",
        "    )\n",
        "\n",
        "@router.delete(\"/conversations/{conversation_id}\")\n",
        "async def delete_conversation(conversation_id: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    await history_service.delete_conversation(conversation_id)\n",
        "    return {\"message\": \"Conversation supprim\u00e9e\"}\n",
        "\n",
        "@router.patch(\"/conversations/{conversation_id}\")\n",
        "async def rename_conversation(conversation_id: str, title: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    updated = await history_service.rename_conversation(conversation_id, title)\n",
        "    return ConversationResponse(\n",
        "        id=updated.id,\n",
        "        title=updated.title,\n",
        "        mode=updated.mode,\n",
        "        created_at=updated.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in updated.messages\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKfgqZJ7l_vX",
        "outputId": "453117c8-7237-48b1-aa30-3d0a375d6e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from chat_router import router\n",
        "\n",
        "app = FastAPI(title=\"AI Conversation Backend - Colab\")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup():\n",
        "    from chat_service import chat_service\n",
        "    chat_service.load_models()\n",
        "    print(\"\u2705 Mod\u00e8les pr\u00e9-charg\u00e9s\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "app.include_router(router)\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Backend running on Colab with ChromaDB + Qwen 7B\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a38f0e757496465da7d4b96da1e054b0",
            "4970c7ebc6cf4358883e93625e11158a",
            "fc2d5854689e473aa469742de85902e1",
            "545e906bdabd473682f18e9c92b88dbd",
            "a243d9e56bac45028c46637398d6a1c5",
            "c5882112c49d47eeb7f02eccac724c07",
            "3102ad4cc6fe48c7bd28865530192b52",
            "f0850ee9002442dd8c5461d6adc5ae71",
            "4249b9602b714b1cbe3097b77134d587",
            "49fd7ed20baa4b75814c99a5babbdc31",
            "be3adc05660e41a6854be4ee7a6feebd"
          ],
          "height": 503
        },
        "id": "U635lwBemFBZ",
        "outputId": "83647bf4-10e5-46a7-9992-bad42970a076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 ChromaDB initialis\u00e9: /content/drive/MyDrive/conversation_db\n",
            "\n",
            "============================================================\n",
            "\ud83c\udf10 URL PUBLIQUE DE TON BACKEND:\n",
            "   NgrokTunnel: \"https://overfew-subtriquetrous-jae.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
            "============================================================\n",
            "\n",
            "\ud83d\ude80 Serveur RELOAD\u00c9 et en cours d'ex\u00e9cution... (Logs activ\u00e9s)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [52858]\n",
            "INFO:     Waiting for application startup.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcc2 Chargement Qwen 7B depuis cache Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a38f0e757496465da7d4b96da1e054b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-11-27T16:22:04+0000 lvl=warn msg=\"failed to open private leg\" id=3daaec61f2ca privaddr=localhost:8000 err=\"dial tcp [::1]:8000: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Qwen 7B charg\u00e9\n",
            "\ud83d\udd04 Chargement mod\u00e8le embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Mod\u00e8le embeddings charg\u00e9\n",
            "\u2705 Mod\u00e8les pr\u00e9-charg\u00e9s\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import asyncio\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# --- FORCER LE RECHARGEMENT DES MODULES ---\n",
        "# Cela oblige Python \u00e0 relire tes fichiers modifi\u00e9s (chat_service.py, etc.)\n",
        "import chat_service\n",
        "import chat_router\n",
        "import history_service\n",
        "import main\n",
        "\n",
        "importlib.reload(chat_service)   # Recharge le service avec le chrono\n",
        "importlib.reload(history_service)\n",
        "importlib.reload(chat_router)\n",
        "importlib.reload(main)           # Recharge l'app FastAPI avec les nouveaux liens\n",
        "# ------------------------------------------\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# D\u00e9marrer ngrok\n",
        "# Note: Si ngrok est d\u00e9j\u00e0 lanc\u00e9, il peut donner une erreur, ignore-la ou kill le process avant\n",
        "try:\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\ud83c\udf10 URL PUBLIQUE DE TON BACKEND:\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(\"=\"*60)\n",
        "except:\n",
        "    print(\"Ngrok d\u00e9j\u00e0 actif ou erreur de connexion\")\n",
        "\n",
        "print(\"\\n\ud83d\ude80 Serveur RELOAD\u00c9 et en cours d'ex\u00e9cution... (Logs activ\u00e9s)\\n\")\n",
        "\n",
        "# D\u00e9marrer FastAPI\n",
        "# On pointe directement sur l'objet app recharg\u00e9 pour \u00eatre s\u00fbr\n",
        "config = uvicorn.Config(main.app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "await server.serve()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a38f0e757496465da7d4b96da1e054b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4970c7ebc6cf4358883e93625e11158a",
              "IPY_MODEL_fc2d5854689e473aa469742de85902e1",
              "IPY_MODEL_545e906bdabd473682f18e9c92b88dbd"
            ],
            "layout": "IPY_MODEL_a243d9e56bac45028c46637398d6a1c5"
          }
        },
        "4970c7ebc6cf4358883e93625e11158a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5882112c49d47eeb7f02eccac724c07",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_3102ad4cc6fe48c7bd28865530192b52",
            "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
          }
        },
        "fc2d5854689e473aa469742de85902e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0850ee9002442dd8c5461d6adc5ae71",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4249b9602b714b1cbe3097b77134d587",
            "value": 4
          }
        },
        "545e906bdabd473682f18e9c92b88dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49fd7ed20baa4b75814c99a5babbdc31",
            "placeholder": "\u200b",
            "style": "IPY_MODEL_be3adc05660e41a6854be4ee7a6feebd",
            "value": "\u20074/4\u2007[02:32&lt;00:00,\u200732.34s/it]"
          }
        },
        "a243d9e56bac45028c46637398d6a1c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5882112c49d47eeb7f02eccac724c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3102ad4cc6fe48c7bd28865530192b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0850ee9002442dd8c5461d6adc5ae71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4249b9602b714b1cbe3097b77134d587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49fd7ed20baa4b75814c99a5babbdc31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3adc05660e41a6854be4ee7a6feebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}