{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZKR4kfmkv5E",
        "outputId": "95c4725c-acdc-4eb9-d553-79590efdaf63"
      },
      "outputs": [],
      "source": [
        "!pip install -q fastapi uvicorn pyngrok chromadb sentence-transformers\n",
        "!pip install -q transformers accelerate bitsandbytes torch\n",
        "!pip install -q pydantic-settings python-dotenv httpx\n",
        "print('‚úÖ D√©pendances install√©es')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8x2l12Pk4gZ",
        "outputId": "f7e079f2-1077-4435-c32b-d9017e67e2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mont√©\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('‚úÖ Google Drive mont√©')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOFO8cGpk9Ed",
        "outputId": "421d4504-8c6f-48a8-cd6c-6f8178081929"
      },
      "outputs": [],
      "source": [
        "# Installer ngrok via snap (plus fiable sur Colab)\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update -qq\n",
        "!sudo apt install ngrok -qq\n",
        "\n",
        "# Configurer le token\n",
        "!ngrok authtoken 361IzWAOvuvUMfJjCWN0hOTPALb_7SPYyHQrzZkdHiXtNrnME\n",
        "\n",
        "# Tuer les processus ngrok existants\n",
        "!pkill -f ngrok\n",
        "\n",
        "print('‚úÖ ngrok install√© et configur√©')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HALHW6M8lDL7",
        "outputId": "8b789c7a-e4c5-44e0-d9e2-277fa5c718c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting database.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile database.py\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "DB_PATH = \"/content/drive/MyDrive/conversation_db\"\n",
        "\n",
        "client = chromadb.PersistentClient(\n",
        "    path=DB_PATH,\n",
        "    settings=Settings(anonymized_telemetry=False)\n",
        ")\n",
        "\n",
        "conversations_collection = client.get_or_create_collection(\n",
        "    name=\"conversations\",\n",
        "    metadata={\"description\": \"Conversations metadata\"}\n",
        ")\n",
        "\n",
        "messages_collection = client.get_or_create_collection(\n",
        "    name=\"messages\",\n",
        "    metadata={\"description\": \"Messages with embeddings\"}\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ ChromaDB initialis√©: {DB_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ7qSs6NlGwy",
        "outputId": "cea3c01f-dc94-458c-c617-bdbed2de83fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile models.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class Conversation:\n",
        "    id: str\n",
        "    title: str\n",
        "    mode: str\n",
        "    created_at: str\n",
        "    messages: List['Message'] = field(default_factory=list)\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    id: str\n",
        "    conversation_id: str\n",
        "    sender: str\n",
        "    content: str\n",
        "    timestamp: str\n",
        "    suggestions: Optional[List[str]] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfRsXOs9lc3d",
        "outputId": "da34a86f-d6f0-4022-a6b5-efbe2f31fee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting schemas.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile schemas.py\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "\n",
        "class MessageCreate(BaseModel):\n",
        "    content: str\n",
        "    stream: bool = False\n",
        "\n",
        "class MessageResponse(BaseModel):\n",
        "    id: str\n",
        "    sender: str\n",
        "    content: str\n",
        "    timestamp: str\n",
        "    suggestions: Optional[List[str]] = None\n",
        "\n",
        "class ConversationCreate(BaseModel):\n",
        "    mode: str = \"user_initiated\"\n",
        "    title: Optional[str] = \"New Conversation\"\n",
        "\n",
        "class ConversationResponse(BaseModel):\n",
        "    id: str\n",
        "    title: str\n",
        "    mode: str\n",
        "    created_at: str\n",
        "    messages: List[MessageResponse] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfmiWeoSlhFE",
        "outputId": "33553c74-692a-4b7a-bd53-b561bcc8cf30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting history_service.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile history_service.py\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "from models import Conversation, Message\n",
        "from schemas import ConversationCreate\n",
        "from database import conversations_collection, messages_collection\n",
        "\n",
        "class HistoryService:\n",
        "    async def create_conversation(self, conversation_data: ConversationCreate) -> Conversation:\n",
        "        conv_id = str(uuid.uuid4())\n",
        "        created_at = datetime.utcnow().isoformat()\n",
        "\n",
        "        conversations_collection.add(\n",
        "            ids=[conv_id],\n",
        "            metadatas=[{\n",
        "                \"title\": conversation_data.title,\n",
        "                \"mode\": conversation_data.mode,\n",
        "                \"created_at\": created_at\n",
        "            }],\n",
        "            documents=[f\"{conversation_data.title}\"]\n",
        "        )\n",
        "\n",
        "        return Conversation(\n",
        "            id=conv_id,\n",
        "            title=conversation_data.title,\n",
        "            mode=conversation_data.mode,\n",
        "            created_at=created_at,\n",
        "            messages=[]\n",
        "        )\n",
        "\n",
        "    async def get_conversation(self, conversation_id: str) -> Conversation:\n",
        "        conv_result = conversations_collection.get(ids=[conversation_id])\n",
        "        if not conv_result['ids']:\n",
        "            return None\n",
        "\n",
        "        metadata = conv_result['metadatas'][0]\n",
        "        messages = await self.get_messages(conversation_id)\n",
        "\n",
        "        return Conversation(\n",
        "            id=conversation_id,\n",
        "            title=metadata['title'],\n",
        "            mode=metadata['mode'],\n",
        "            created_at=metadata['created_at'],\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "    async def add_message(self, conversation_id: str, sender: str, content: str,\n",
        "                         embedding: List[float], suggestions=None) -> Message:\n",
        "        msg_id = str(uuid.uuid4())\n",
        "        timestamp = datetime.utcnow().isoformat()\n",
        "\n",
        "        metadata = {\n",
        "            \"conversation_id\": conversation_id,\n",
        "            \"sender\": sender,\n",
        "            \"timestamp\": timestamp\n",
        "        }\n",
        "\n",
        "        if suggestions and len(suggestions) > 0:\n",
        "            metadata[\"suggestions\"] = \",\".join(suggestions)\n",
        "\n",
        "        messages_collection.add(\n",
        "            ids=[msg_id],\n",
        "            embeddings=[embedding],\n",
        "            metadatas=[metadata],\n",
        "            documents=[content]\n",
        "        )\n",
        "\n",
        "        return Message(\n",
        "            id=msg_id,\n",
        "            conversation_id=conversation_id,\n",
        "            sender=sender,\n",
        "            content=content,\n",
        "            timestamp=timestamp,\n",
        "            suggestions=suggestions\n",
        "        )\n",
        "\n",
        "    async def get_messages(self, conversation_id: str) -> List[Message]:\n",
        "        results = messages_collection.get(\n",
        "            where={\"conversation_id\": conversation_id}\n",
        "        )\n",
        "\n",
        "        messages = []\n",
        "        for i, msg_id in enumerate(results['ids']):\n",
        "            metadata = results['metadatas'][i]\n",
        "            sugg_str = metadata.get('suggestions')\n",
        "            suggestions = sugg_str.split(',') if sugg_str else None\n",
        "\n",
        "            messages.append(Message(\n",
        "                id=msg_id,\n",
        "                conversation_id=conversation_id,\n",
        "                sender=metadata['sender'],\n",
        "                content=results['documents'][i],\n",
        "                timestamp=metadata['timestamp'],\n",
        "                suggestions=suggestions\n",
        "            ))\n",
        "\n",
        "        messages.sort(key=lambda x: x.timestamp)\n",
        "        return messages\n",
        "\n",
        "    async def list_conversations(self, skip: int = 0, limit: int = 100) -> List[Conversation]:\n",
        "        results = conversations_collection.get()\n",
        "\n",
        "        conversations = []\n",
        "        for i, conv_id in enumerate(results['ids']):\n",
        "            metadata = results['metadatas'][i]\n",
        "            conversations.append(Conversation(\n",
        "                id=conv_id,\n",
        "                title=metadata['title'],\n",
        "                mode=metadata['mode'],\n",
        "                created_at=metadata['created_at'],\n",
        "                messages=[]\n",
        "            ))\n",
        "\n",
        "        conversations.sort(key=lambda x: x.created_at, reverse=True)\n",
        "        return conversations[skip:skip+limit]\n",
        "\n",
        "    async def delete_conversation(self, conversation_id: str):\n",
        "        # Supprimer les messages\n",
        "        messages_collection.delete(where={\"conversation_id\": conversation_id})\n",
        "        # Supprimer la conversation\n",
        "        conversations_collection.delete(ids=[conversation_id])\n",
        "\n",
        "    async def rename_conversation(self, conversation_id: str, title: str) -> Conversation:\n",
        "        conv = await self.get_conversation(conversation_id)\n",
        "        if conv:\n",
        "            conversations_collection.update(\n",
        "                ids=[conversation_id],\n",
        "                metadatas=[{\n",
        "                    \"title\": title,\n",
        "                    \"mode\": conv.mode,\n",
        "                    \"created_at\": conv.created_at\n",
        "                }],\n",
        "                documents=[title]\n",
        "            )\n",
        "            conv.title = title\n",
        "        return conv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile chat_service.py\n",
        "from typing import List, Tuple, Optional\n",
        "from models import Message\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# CONFIGURATION CACHE\n",
        "CACHE_DIR = \"/content/drive/MyDrive/huggingface_cache\"\n",
        "os.environ['HF_HOME'] = CACHE_DIR\n",
        "os.environ['TRANSFORMERS_CACHE'] = CACHE_DIR\n",
        "\n",
        "class ChatService:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.embedding_model = None\n",
        "\n",
        "    def load_models(self):\n",
        "        if not os.path.exists(CACHE_DIR):\n",
        "            os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "            print(f\"üìÅ Dossier de cache cr√©√© : {CACHE_DIR}\")\n",
        "\n",
        "        if self.model is None:\n",
        "            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "\n",
        "            print(f\"üîÑ V√©rification du mod√®le Qwen dans {CACHE_DIR}...\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                cache_dir=CACHE_DIR\n",
        "            )\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",\n",
        "                cache_dir=CACHE_DIR\n",
        "            )\n",
        "            print(\"‚úÖ Qwen 7B charg√©\")\n",
        "\n",
        "        if self.embedding_model is None:\n",
        "            print(\"üîÑ Chargement mod√®le embeddings...\")\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder=CACHE_DIR)\n",
        "            print(\"‚úÖ Mod√®le embeddings charg√©\")\n",
        "\n",
        "    def generate_embedding(self, text: str) -> List[float]:\n",
        "        if self.embedding_model is None:\n",
        "            self.load_models()\n",
        "        return self.embedding_model.encode(text).tolist()\n",
        "\n",
        "    async def generate_response_stream(self, history: List[Message]):\n",
        "        from transformers import TextIteratorStreamer\n",
        "        from threading import Thread\n",
        "\n",
        "        if self.model is None:\n",
        "            self.load_models()\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Tu es un assistant IA intelligent et amical. R√©ponds de mani√®re naturelle et concise en fran√ßais.\"}\n",
        "        ]\n",
        "        for msg in history:\n",
        "            role = \"user\" if msg.sender == \"user\" else \"assistant\"\n",
        "            messages.append({\"role\": role, \"content\": msg.content})\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer([text], return_tensors=\"pt\", truncation=True, max_length=1024).to(self.model.device)\n",
        "\n",
        "        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "        generation_kwargs = dict(\n",
        "            inputs,\n",
        "            streamer=streamer,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
        "        thread.start()\n",
        "\n",
        "        token_count = 0\n",
        "        first_token_received = False\n",
        "\n",
        "        for new_text in streamer:\n",
        "            if not first_token_received:\n",
        "                latency = time.time() - start_time\n",
        "                print(f\"\\n‚è±Ô∏è LATENCE (Temps avant 1er mot) : {latency:.4f} secondes\")\n",
        "                first_token_received = True\n",
        "\n",
        "            token_count += 1\n",
        "            yield new_text\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time = end_time - start_time\n",
        "        tokens_per_sec = token_count / total_time if total_time > 0 else 0\n",
        "\n",
        "        print(f\"‚ö° VITESSE DE GENERATION : {tokens_per_sec:.2f} tokens/seconde\")\n",
        "        print(f\"üìù Total tokens (approx stream) : {token_count}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    async def generate_response(self, history: List[Message]) -> Tuple[str, Optional[List[str]]]:\n",
        "        full_response = \"\"\n",
        "        async for token in self.generate_response_stream(history):\n",
        "            full_response += token\n",
        "        return full_response, None\n",
        "\n",
        "chat_service = ChatService()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile chat_router.py\n",
        "from fastapi import APIRouter, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from typing import List\n",
        "from schemas import ConversationCreate, ConversationResponse, MessageCreate, MessageResponse\n",
        "from history_service import HistoryService\n",
        "from chat_service import chat_service\n",
        "\n",
        "router = APIRouter()\n",
        "history_service = HistoryService()\n",
        "\n",
        "@router.post(\"/conversations\", response_model=ConversationResponse)\n",
        "async def create_conversation(conversation: ConversationCreate):\n",
        "    conv = await history_service.create_conversation(conversation)\n",
        "\n",
        "    if conversation.mode == \"ai_initiated\":\n",
        "        greeting = \"Bonjour ! Je suis votre assistant IA. Comment puis-je vous aider aujourd'hui ?\"\n",
        "        greeting_embedding = chat_service.generate_embedding(greeting)\n",
        "        await history_service.add_message(conv.id, \"ai\", greeting, greeting_embedding, None)\n",
        "        conv = await history_service.get_conversation(conv.id)\n",
        "\n",
        "    return ConversationResponse(\n",
        "        id=conv.id,\n",
        "        title=conv.title,\n",
        "        mode=conv.mode,\n",
        "        created_at=conv.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in conv.messages\n",
        "        ]\n",
        "    )\n",
        "\n",
        "@router.get(\"/conversations\", response_model=List[ConversationResponse])\n",
        "async def list_conversations():\n",
        "    convs = await history_service.list_conversations()\n",
        "    return [\n",
        "        ConversationResponse(\n",
        "            id=c.id,\n",
        "            title=c.title,\n",
        "            mode=c.mode,\n",
        "            created_at=c.created_at,\n",
        "            messages=[]\n",
        "        ) for c in convs\n",
        "    ]\n",
        "\n",
        "@router.get(\"/conversations/{conversation_id}\", response_model=ConversationResponse)\n",
        "async def get_conversation(conversation_id: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    return ConversationResponse(\n",
        "        id=conv.id,\n",
        "        title=conv.title,\n",
        "        mode=conv.mode,\n",
        "        created_at=conv.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in conv.messages\n",
        "        ]\n",
        "    )\n",
        "\n",
        "@router.post(\"/conversations/{conversation_id}/messages\", response_model=MessageResponse)\n",
        "async def send_message(conversation_id: str, message: MessageCreate):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    user_embedding = chat_service.generate_embedding(message.content)\n",
        "    user_msg = await history_service.add_message(conversation_id, \"user\", message.content, user_embedding, None)\n",
        "\n",
        "    history = await history_service.get_messages(conversation_id)\n",
        "\n",
        "    if message.stream:\n",
        "        async def event_generator():\n",
        "            full_response = \"\"\n",
        "            async for token in chat_service.generate_response_stream(history):\n",
        "                full_response += token\n",
        "                yield token\n",
        "            \n",
        "            ai_embedding = chat_service.generate_embedding(full_response)\n",
        "            await history_service.add_message(conversation_id, \"ai\", full_response, ai_embedding, None)\n",
        "\n",
        "        return StreamingResponse(event_generator(), media_type=\"text/plain\")\n",
        "\n",
        "    ai_response, suggestions = await chat_service.generate_response(history)\n",
        "\n",
        "    ai_embedding = chat_service.generate_embedding(ai_response)\n",
        "    ai_msg = await history_service.add_message(conversation_id, \"ai\", ai_response, ai_embedding, None)\n",
        "\n",
        "    return MessageResponse(\n",
        "        id=ai_msg.id,\n",
        "        sender=ai_msg.sender,\n",
        "        content=ai_msg.content,\n",
        "        timestamp=ai_msg.timestamp,\n",
        "        suggestions=ai_msg.suggestions\n",
        "    )\n",
        "\n",
        "@router.delete(\"/conversations/{conversation_id}\")\n",
        "async def delete_conversation(conversation_id: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    await history_service.delete_conversation(conversation_id)\n",
        "    return {\"message\": \"Conversation supprim√©e\"}\n",
        "\n",
        "@router.patch(\"/conversations/{conversation_id}\")\n",
        "async def rename_conversation(conversation_id: str, title: str):\n",
        "    conv = await history_service.get_conversation(conversation_id)\n",
        "    if not conv:\n",
        "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
        "\n",
        "    updated = await history_service.rename_conversation(conversation_id, title)\n",
        "    return ConversationResponse(\n",
        "        id=updated.id,\n",
        "        title=updated.title,\n",
        "        mode=updated.mode,\n",
        "        created_at=updated.created_at,\n",
        "        messages=[\n",
        "            MessageResponse(\n",
        "                id=m.id,\n",
        "                sender=m.sender,\n",
        "                content=m.content,\n",
        "                timestamp=m.timestamp,\n",
        "                suggestions=m.suggestions\n",
        "            ) for m in updated.messages\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKfgqZJ7l_vX",
        "outputId": "453117c8-7237-48b1-aa30-3d0a375d6e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from chat_router import router\n",
        "\n",
        "app = FastAPI(title=\"AI Conversation Backend - Colab\")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup():\n",
        "    from chat_service import chat_service\n",
        "    chat_service.load_models()\n",
        "    print(\"‚úÖ Mod√®les pr√©-charg√©s\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "app.include_router(router)\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Backend running on Colab with ChromaDB + Qwen 7B\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "a38f0e757496465da7d4b96da1e054b0",
            "4970c7ebc6cf4358883e93625e11158a",
            "fc2d5854689e473aa469742de85902e1",
            "545e906bdabd473682f18e9c92b88dbd",
            "a243d9e56bac45028c46637398d6a1c5",
            "c5882112c49d47eeb7f02eccac724c07",
            "3102ad4cc6fe48c7bd28865530192b52",
            "f0850ee9002442dd8c5461d6adc5ae71",
            "4249b9602b714b1cbe3097b77134d587",
            "49fd7ed20baa4b75814c99a5babbdc31",
            "be3adc05660e41a6854be4ee7a6feebd"
          ]
        },
        "id": "U635lwBemFBZ",
        "outputId": "83647bf4-10e5-46a7-9992-bad42970a076"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import asyncio\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# --- FORCER LE RECHARGEMENT DES MODULES ---\n",
        "# Cela oblige Python √† relire tes fichiers modifi√©s (chat_service.py, etc.)\n",
        "import chat_service\n",
        "import chat_router\n",
        "import history_service\n",
        "import main\n",
        "\n",
        "importlib.reload(chat_service)   # Recharge le service avec le chrono\n",
        "importlib.reload(history_service)\n",
        "importlib.reload(chat_router)\n",
        "importlib.reload(main)           # Recharge l'app FastAPI avec les nouveaux liens\n",
        "# ------------------------------------------\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# D√©marrer ngrok\n",
        "# Note: Si ngrok est d√©j√† lanc√©, il peut donner une erreur, ignore-la ou kill le process avant\n",
        "try:\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üåê URL PUBLIQUE DE TON BACKEND:\")\n",
        "    print(f\"   {public_url}\")\n",
        "    print(\"=\"*60)\n",
        "except:\n",
        "    print(\"Ngrok d√©j√† actif ou erreur de connexion\")\n",
        "\n",
        "print(\"\\nüöÄ Serveur RELOAD√â et en cours d'ex√©cution... (Logs activ√©s)\\n\")\n",
        "\n",
        "# D√©marrer FastAPI\n",
        "# On pointe directement sur l'objet app recharg√© pour √™tre s√ªr\n",
        "config = uvicorn.Config(main.app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "await server.serve()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3102ad4cc6fe48c7bd28865530192b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4249b9602b714b1cbe3097b77134d587": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4970c7ebc6cf4358883e93625e11158a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5882112c49d47eeb7f02eccac724c07",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3102ad4cc6fe48c7bd28865530192b52",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "49fd7ed20baa4b75814c99a5babbdc31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545e906bdabd473682f18e9c92b88dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49fd7ed20baa4b75814c99a5babbdc31",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be3adc05660e41a6854be4ee7a6feebd",
            "value": "‚Äá4/4‚Äá[02:32&lt;00:00,‚Äá32.34s/it]"
          }
        },
        "a243d9e56bac45028c46637398d6a1c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38f0e757496465da7d4b96da1e054b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4970c7ebc6cf4358883e93625e11158a",
              "IPY_MODEL_fc2d5854689e473aa469742de85902e1",
              "IPY_MODEL_545e906bdabd473682f18e9c92b88dbd"
            ],
            "layout": "IPY_MODEL_a243d9e56bac45028c46637398d6a1c5"
          }
        },
        "be3adc05660e41a6854be4ee7a6feebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5882112c49d47eeb7f02eccac724c07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0850ee9002442dd8c5461d6adc5ae71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2d5854689e473aa469742de85902e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0850ee9002442dd8c5461d6adc5ae71",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4249b9602b714b1cbe3097b77134d587",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
